{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13705e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc65e7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9db34e",
   "metadata": {},
   "source": [
    "## Observability Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf6e89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tituslim/Documents/Personal Learning Folder/Personal Projects/Observe/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Phoenix Project: multi-step-rag\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: localhost:4317\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  ‚ö†Ô∏è WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "from phoenix.otel import register\n",
    "\n",
    "tracer_provider = register(\n",
    "    project_name=\"multi-step-rag\", \n",
    ")\n",
    "LlamaIndexInstrumentor().instrument(\n",
    "    tracer_provider=tracer_provider,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e323d6c5",
   "metadata": {},
   "source": [
    "## RAG application setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddf3b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    Settings, \n",
    "    SimpleDirectoryReader, \n",
    "    VectorStoreIndex, \n",
    ")\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "Settings.llm = Ollama(model=\"gemma3\", temperature=0, request_timeout=60000)\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"mxbai-embed-large:latest\")\n",
    "eval_llm = Ollama(model=\"gpt-oss:20b\", request_timeout=60000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04910706",
   "metadata": {},
   "source": [
    "### Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b5d7df",
   "metadata": {},
   "source": [
    "Create the vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6145c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(input_dir=\"./docs\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554fb660",
   "metadata": {},
   "source": [
    "Create the vector index and ingest vectors into PostGres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ed566bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:14:53,757 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:53,819 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:53,873 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:53,928 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:53,984 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,037 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,090 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,143 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,180 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,221 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,276 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,328 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,368 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,420 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,472 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,526 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,577 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,629 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,682 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,734 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,779 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,832 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,867 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,918 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,971 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,027 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,079 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,121 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,173 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,220 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,273 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,316 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,369 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,396 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,448 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,501 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,549 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,601 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,655 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,707 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,760 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,812 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,868 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,920 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,972 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,025 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,076 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,127 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,161 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,214 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,256 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,308 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,360 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,412 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,465 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,518 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,570 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,625 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,677 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,732 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,783 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,835 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,887 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,938 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,990 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,042 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,079 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,134 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,186 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,238 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,290 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,342 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,395 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,448 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,500 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,553 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,608 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,663 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,716 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,767 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,819 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,871 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,923 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,976 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,027 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,079 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,132 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,184 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,237 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,290 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,342 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,379 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,430 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,485 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,539 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,604 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,648 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,702 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,756 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,810 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,846 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,901 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,954 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,006 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,061 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,115 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,169 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,233 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,289 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,344 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,399 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,454 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,476 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,533 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,586 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,637 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,691 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,720 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,743 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,781 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,837 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,891 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,950 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,005 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,058 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,112 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,166 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,222 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,278 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,332 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,389 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,444 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,500 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,556 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,612 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,669 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,724 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,777 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,834 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,888 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,943 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,996 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,050 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,105 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,129 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,155 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,187 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,211 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,235 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,286 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,323 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,360 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,396 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,431 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,469 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,498 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,527 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,579 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,632 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,685 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,749 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,803 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,842 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,896 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,938 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,992 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,047 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,089 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,144 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,198 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,264 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,315 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,370 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,418 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,472 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,528 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,583 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,638 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,683 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ee98f",
   "metadata": {},
   "source": [
    "## The RAG query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0298bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    \"Can you tell me how Alita and MCP Zero can interplay with each other? \"\n",
    "    \"Also, how can GEPA perform better than GRPO even though it's a prompt engineering \"\n",
    "    \"technique that does not rewrite the weights of the LLM?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ece21d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:15:09,831 - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "base_query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca1d0607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:15:10,694 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:14,821 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "response = await base_query_engine.aquery(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f6bcb17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Alita generates MCPs which can be utilized by other agents, enhancing their capabilities and problem-solving abilities. These MCPs, distilled from powerful models like Claude-3.7-Sonnet, bridge the gap in task-processing capabilities between agents utilizing smaller LLMs and those leveraging larger models."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d7b3989",
   "metadata": {},
   "outputs": [],
   "source": [
    "higher_k_query_engine = index.as_query_engine(similarity_top_k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5acce928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:15:19,922 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:26,321 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Alita‚Äôs design centers around leveraging the increasing coding and reasoning capabilities of LLMs. When running Alita on GAIA using GPT-4o-mini, it generates its own MCPs ‚Äì meaning it doesn‚Äôt rely on distilled MCPs from more powerful models like Claude-3.7-Sonnet. The experiment shows that Alita performs significantly worse on GAIA compared to when using GPT-4o-mini. This highlights the critical role of the underlying models‚Äô coding capabilities. \n",
       "\n",
       "GEPA can outperform GRPO, even though it‚Äôs a prompt engineering technique, because it uses a Pareto-based sampling strategy to generate prompts. This approach allows GEPA to explore a broader range of potential solutions and identify the most effective prompts, leading to improved performance. Furthermore, GEPA‚Äôs system-aware crossover strategies can provide large gains, but the optimal budget allocation between mutation and crossover, as well as when to invoke merge needs further study."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = await higher_k_query_engine.aquery(query)\n",
    "display(Markdown(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e8e8735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor.llm_rerank import LLMRerank\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "reranker = LLMRerank(top_n=4)\n",
    "reranker_query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever = index.as_retriever(similarity_top_k=10),\n",
    "    llm = Settings.llm,\n",
    "    node_postprocessors=[reranker]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0d6cf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:15:30,094 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:35,911 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:39,487 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Alita and MCP-Zero address complementary halves of the same problem: MCP-Zero efficiently finds and invokes existing tools, while Alita automatically builds missing tools on-the-fly. When combined, they form a virtuous loop where an agent first actively discovers tools, and if none fit, Alita synthesizes a new one. GEPA achieves optimal test set performance by rapidly adapting and generalizing in compound AI systems, outperforming GRPO by up to 19% while using up to 35x fewer rollouts."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = await reranker_query_engine.aquery(query)\n",
    "display(Markdown(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cde5827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "\n",
    "hyde = HyDEQueryTransform(include_original=True)\n",
    "hyde_query_engine = TransformQueryEngine(reranker_query_engine, query_transform=hyde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80ca573d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:15:54,153 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:54,238 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:54,266 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:59,439 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:16:02,741 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "According to the provided documents, Alita generates MCPs (Mission Control Protocols) which are then reused by other agents, like ODR-smolagents. Specifically, Alita, when using GPT-4o-mini, generates its own MCPs, unlike the experiment in Section 5.1.3 where the agent utilized MCPs distilled from Claude-3.7-Sonnet. GEPA, a prompt optimizer, can outperform GRPO, a reinforcement learning algorithm, because it incorporates natural language reflection to diagnose problems and propose prompt updates, leading to a significant quality gain with fewer rollouts."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = await hyde_query_engine.aquery(query)\n",
    "display(Markdown(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77dcbbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:16:06,838 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[alita-gepa-mcpZero] Q: What is the relationship between Alita and MCP Zero?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:16:07,729 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:16:07,781 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:16:07,806 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:16:15,592 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:16:18,973 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;237;90;200m[alita-gepa-mcpZero] A: Alita autonomously expands its capabilities through continuous MCP integration. It generates MCPs, which are then encapsulated and stored in the MCP Box for future reuse. These MCPs are created through a self-reinforcing cycle where Alita continuously integrates new MCPs, enhancing its overall capabilities.\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[alita-gepa-mcpZero] Q: How do Alita and MCP Zero interact?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:16:19,885 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:16:19,935 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:16:19,961 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:01,261 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;90;149;237m[alita-gepa-mcpZero] A: Empty Response\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[alita-gepa-mcpZero] Q: How does GEPA improve GRPO performance?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:17:02,453 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:02,505 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:02,530 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:08,339 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:11,661 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;11;159;203m[alita-gepa-mcpZero] A: GEPA achieves superior test set performance compared to GRPO on tasks like HotpotQA, IFBench, HoVer, and PUPA by requiring significantly fewer rollouts. Specifically, it matches GRPO‚Äôs best validation scores with 402, 330, 1179, and 306 rollouts, respectively, while achieving up to 78 times greater sample efficiency. Furthermore, the combined GEPA+Merge approach out-performs GRPO by an even wider margin of 21% at a comparable rollout budget.\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:17:12,754 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Alita autonomously expands its capabilities through continuous integration of MCPs. It generates these MCPs and stores them for later use. This creates a cycle of enhancement. GEPA achieves better results than GRPO by using far fewer rollouts to reach similar validation scores, demonstrating greater sample efficiency."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.question_gen import LLMQuestionGenerator\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=hyde_query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"alita-gepa-mcpZero\",\n",
    "            description=\"Use this for specific questions relating to alita, gepa and/or mcp zero\",\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "generator = LLMQuestionGenerator.from_defaults()\n",
    "sub_question_query_engine = SubQuestionQueryEngine(\n",
    "    question_gen=generator,\n",
    "    response_synthesizer=get_response_synthesizer(),\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    use_async=False\n",
    ")\n",
    "\n",
    "response = sub_question_query_engine.query(query)\n",
    "display(Markdown(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15b7b115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.query.query_transform import (\n",
    "    StepDecomposeQueryTransform\n",
    ")\n",
    "from llama_index.core.query_engine import MultiStepQueryEngine\n",
    "\n",
    "transform = StepDecomposeQueryTransform(verbose=True)\n",
    "multi_step_query_engine = MultiStepQueryEngine(\n",
    "    query_engine = sub_question_query_engine,\n",
    "    query_transform = transform,\n",
    "    index_summary = \"Answers questions relating to alita, gepa, and/or mcp zero.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3a848e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:17:21,853 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;33m> Current query: Can you tell me how Alita and MCP Zero can interplay with each other? Also, how can GEPA perform better than GRPO even though it's a prompt engineering technique that does not rewrite the weights of the LLM?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: How does MCP Zero interact with Alita?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:17:22,928 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[alita-gepa-mcpZero] Q: What is the interaction between Alita and MCP Zero?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:17:24,004 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:24,058 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:24,085 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:30,982 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;237;90;200m[alita-gepa-mcpZero] A: Empty Response\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:17:31,315 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:32,016 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;33m> Current query: Can you tell me how Alita and MCP Zero can interplay with each other? Also, how can GEPA perform better than GRPO even though it's a prompt engineering technique that does not rewrite the weights of the LLM?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: How does MCP Zero interact with Alita?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:17:33,121 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[alita-gepa-mcpZero] Q: What is the interaction between Alita and MCP Zero?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:17:34,331 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:34,387 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:34,414 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:41,863 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;237;90;200m[alita-gepa-mcpZero] A: Empty Response\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:17:42,187 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:42,961 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;33m> Current query: Can you tell me how Alita and MCP Zero can interplay with each other? Also, how can GEPA perform better than GRPO even though it's a prompt engineering technique that does not rewrite the weights of the LLM?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: How does MCP Zero interact with Alita?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:17:44,183 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[alita-gepa-mcpZero] Q: What is the interaction between Alita and MCP Zero?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:17:45,468 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:45,523 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:45,552 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:53,328 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;237;90;200m[alita-gepa-mcpZero] A: Empty Response\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:17:53,727 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:54,016 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Empty Response\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = multi_step_query_engine.query(query)\n",
    "display(Markdown(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b5db536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import RichPromptTemplate\n",
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    step,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Event,\n",
    "    Workflow\n",
    ")\n",
    "from pydantic import BaseModel\n",
    "from typing import Annotated\n",
    "\n",
    "check_answer_template = RichPromptTemplate(\"\"\"\n",
    "{% chat role = \"user\" %}\n",
    "This is the original question: \n",
    "<question>\n",
    "    {{ question }}\n",
    "</question>\n",
    "\n",
    "Here are the following questions we have asked:\n",
    "<follow_up_questions>\n",
    "    {{ follow_up_questions }}\n",
    "</follow_up_questions>\n",
    "\n",
    "Here is our current answer:\n",
    "<answer>\n",
    "    {{ answer }}\n",
    "</answer>\n",
    "\n",
    "Does the current answer address the original question? If not, generate\n",
    "a follow-up question such that including the answer to this follow-up question\n",
    "to the current answer we have so far answers the user's original question.\n",
    "{% endchat %}\n",
    "\"\"\")\n",
    "\n",
    "class ShouldContinue(BaseModel):\n",
    "    should_continue: bool\n",
    "    reasoning: Annotated[str, \"Whether the current answer answers the question\"]\n",
    "\n",
    "class ConsolidateEvent(Event):\n",
    "    original_question: str\n",
    "    current_response: str\n",
    "    new_response: str\n",
    "    follow_up_questions: list[str]\n",
    "\n",
    "class CheckAnswerEvent(Event):\n",
    "    original_question: str\n",
    "    follow_up_questions: list[str]\n",
    "    response: str\n",
    "\n",
    "class ContinueEvent(Event):\n",
    "    original_question: str\n",
    "    current_answer: str\n",
    "    follow_up_questions: list[str]\n",
    "    reason_to_continue: str\n",
    "\n",
    "class AskQueryEvent(Event):\n",
    "    query: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d86db60",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Settings.llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb482561",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStepRAG(Workflow):    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.llm = Settings.llm        \n",
    "    \n",
    "    @step\n",
    "    async def query_step(\n",
    "        self, ctx: Context, ev: StartEvent | AskQueryEvent\n",
    "    ) -> CheckAnswerEvent | ConsolidateEvent:\n",
    "        \n",
    "        query = ev.get('query')\n",
    "        follow_up_questions = ev.get('follow_up_questions', [])\n",
    "        response = await sub_question_query_engine.query(query)\n",
    "        current_response = await ctx.get(\"current_answer\", None)\n",
    "        \n",
    "        if current_response:\n",
    "            return ConsolidateEvent(\n",
    "                original_question=query,\n",
    "                current_response=current_response,\n",
    "                new_response=response.response,\n",
    "                follow_up_questions=follow_up_questions\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            await ctx.set(\"current_answer\", current_response)\n",
    "            return CheckAnswerEvent(\n",
    "                original_question = query,\n",
    "                follow_up_questions=follow_up_questions,\n",
    "                response = response.response\n",
    "            )\n",
    "    \n",
    "    @step\n",
    "    async def consolidate_response(\n",
    "        self, ctx: Context, ev: ConsolidateEvent\n",
    "    ) -> CheckAnswerEvent:\n",
    "        \n",
    "        follow_up_questions = ev.get('follow_up_questions')\n",
    "        original_question = ev.get('original_question')\n",
    "        current_response = ev.get('current_response')\n",
    "        new_response = ev.get('new_response')\n",
    "        response = await self.llm.acomplete(\n",
    "            f\"\"\"\n",
    "            This is the question we're trying to answer: {original_question}\n",
    "            \n",
    "            Here is the answer we have so far: {current_response}\n",
    "            \n",
    "            This is an additional component to make our current answer more complete: {new_response}\n",
    "            \n",
    "            Generate a coherent answer based on our current answer and the additional component.\n",
    "            \"\"\"\n",
    "        )\n",
    "        await ctx.set(\"current_answer\", response.text)\n",
    "        return CheckAnswerEvent(\n",
    "            original_question = query,\n",
    "            follow_up_questions=follow_up_questions,\n",
    "            response = response.text\n",
    "        )\n",
    "    \n",
    "    @step\n",
    "    async def check_answer_step(\n",
    "        self, ctx: Context, ev: CheckAnswerEvent\n",
    "    ) -> ContinueEvent | StopEvent:\n",
    "        original_question = ev.get('original_question')\n",
    "        follow_up_questions = ev.get('follow_up_questions')\n",
    "        current_answer = ev.get(\"response\")\n",
    "        check_answer_template.format(\n",
    "            question=original_question, \n",
    "            follow_up_questions = follow_up_questions,\n",
    "            answer = current_answer\n",
    "        )\n",
    "        result = self.llm.structured_predict(ShouldContinue, check_answer_template)\n",
    "        if result.should_continue:\n",
    "            return ContinueEvent(\n",
    "                current_answer = current_answer,\n",
    "                original_question = original_question,\n",
    "                follow_up_questions = follow_up_questions,\n",
    "                reason_to_continue = result.reasoning\n",
    "            )\n",
    "        return StopEvent(result = current_answer)\n",
    "    \n",
    "    @step \n",
    "    async def generate_follow_up_question(\n",
    "        self, ctx: Context, ev: ContinueEvent\n",
    "    ) -> AskQueryEvent:\n",
    "        original_question = ev.get(\"original_question\")\n",
    "        current_response = ev.get(\"current_answer\")\n",
    "        \n",
    "        result = await llm.acomplete(\n",
    "             f\"\"\"\n",
    "            This is the question we're trying to answer: {original_question}\n",
    "            \n",
    "            Here is the answer we have so far: {current_response}\n",
    "            \n",
    "            We've not fully addressed the question yet. Generate a follow-up question\n",
    "            so that the answer to this question will address the original question once\n",
    "            combined with our current response.\n",
    "            \"\"\"\n",
    "        )\n",
    "        return AskQueryEvent(query = result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a002ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_step_query_engine.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(\n",
    "    MultiStepRAG(),\n",
    "    filename=\"multi_step_query_engine.html\",\n",
    "    # Optional, can limit long event names in your workflow\n",
    "    # Can help with readability\n",
    "    # max_label_length=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de880364",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_step_query_engine = MultiStepRAG(timeout=6000)\n",
    "ctx = Context(multi_step_query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106bd409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e39e8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here‚Äôs how the two topics fit together, based on the provided material:\n",
       "\n",
       "- Interplay between Alita and MCP-Zero\n",
       "  - What each does\n",
       "    - MCP-Zero is a tool-discovery engine: it actively searches for existing tools and capabilities across resources, and invokes them when suitable. It focuses on maximizing tool discovery and reuse.\n",
       "    - Alita is a generalist agent framework that evolves capabilities by generating and refining task-related model context protocols (MCPs) from open-source material. It aims to synthesize and reuse external capabilities with minimal upfront handcrafting.\n",
       "  - How they work together\n",
       "    - They form a complementary loop: first, MCP-Zero tries to find and invoke existing tools to tackle the agent‚Äôs tasks.\n",
       "    - If no suitable tool is found, Alita‚Äôs workflow can be engaged to synthesize a new tool by generating a new MCP tailored to the task, effectively creating new capabilities.\n",
       "    - The newly created tool (and its MCP) can then be registered and made available to the community, enriching the tool ecosystem for future tasks.\n",
       "  - Why this is powerful\n",
       "    - This pairing balances discovery and creation: MCP-Zero maximizes what already exists, while Alita drives scalable self-evolution by producing and integrating new tools via MCPs.\n",
       "    - The combination supports broader generalization across domains: semantic grounding via MCPs helps clarify tool semantics, enabling reliable tool use and faster adaptation to new tasks.\n",
       "\n",
       "- Why GEPA can beat GRPO without changing LLM weights\n",
       "  - Core idea\n",
       "    - GEPA is a reflective prompt evolution method that optimizes prompts (system-level instructions and tool-use guidance) rather than updating model weights. It leverages natural-language reflection to diagnose issues, propose prompt updates, and combine lessons from multiple attempts.\n",
       "  - Why it can outperform weight-space RL (GRPO)\n",
       "    - High sample efficiency: GEPA can achieve large performance gains with far fewer rollouts (up to 35x fewer) by learning mainly from improved prompts and reflections rather than policy updates.\n",
       "    - Better use of feedback: GEPA uses a reflection-based process to generate high-quality, task-relevant learning signals from each rollout, guiding prompt evolution more effectively than scalar reward signals alone.\n",
       "    - Diverse, Pareto-guided exploration: GEPA uses Pareto-based candidate sampling to maintain diversity among evolving prompts, avoiding local optima that can trap strategies that always pick the current best candidate.\n",
       "    - Systematic prompt combination: The approach includes mutation and a system-aware merge step, which can combine complementary prompt strategies from different evolutionary lineages to produce stronger prompts.\n",
       "    - Evidence across tasks/models: In experiments, GEPA and its variant GEPA+Merge outperformed GRPO by up to about 19% on some tasks, with substantial reductions in rollouts required, and often matched or exceeded GRPO‚Äôs best validation scores with far fewer learning signals.\n",
       "  - Practical takeaway\n",
       "    - The gains come from optimizing the prompts and the learning dynamics (how prompts are mutated, merged, and selected) rather than from changing LLM weights. This makes GEPA a highly sample-efficient way to improve downstream performance for complex, modular AI systems where prompts and system behavior are crucial.\n",
       "\n",
       "If you want, I can summarize how to architect a system that combines Alita with MCP-Zero in a concrete workflow, and separately outline a GEPA-inspired prompt-evolution protocol you could pilot for a given task."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "openai_query_engine = index.as_query_engine(similariry_top_k=20, llm=OpenAI(model=\"gpt-5-nano\", temperature=0))\n",
    "display(Markdown(response.response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Observe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
