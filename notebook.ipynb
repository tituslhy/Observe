{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13705e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc65e7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9db34e",
   "metadata": {},
   "source": [
    "## Observability Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf6e89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tituslim/Documents/Personal Learning Folder/Personal Projects/Observe/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Phoenix Project: multi-step-rag\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: localhost:4317\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  ‚ö†Ô∏è WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "from phoenix.otel import register\n",
    "\n",
    "tracer_provider = register(\n",
    "    project_name=\"multi-step-rag\", \n",
    ")\n",
    "LlamaIndexInstrumentor().instrument(\n",
    "    tracer_provider=tracer_provider,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e323d6c5",
   "metadata": {},
   "source": [
    "## RAG application setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddf3b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    Settings, \n",
    "    SimpleDirectoryReader, \n",
    "    VectorStoreIndex, \n",
    ")\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "Settings.llm = Ollama(model=\"gemma3\", temperature=0, request_timeout=60000)\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"mxbai-embed-large:latest\")\n",
    "eval_llm = Ollama(model=\"gpt-oss:20b\", request_timeout=60000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04910706",
   "metadata": {},
   "source": [
    "### Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b5d7df",
   "metadata": {},
   "source": [
    "Create the vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6145c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(input_dir=\"./docs\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554fb660",
   "metadata": {},
   "source": [
    "Create the vector index and ingest vectors into PostGres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ed566bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:14:53,757 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:53,819 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:53,873 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:53,928 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:53,984 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,037 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,090 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,143 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,180 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,221 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,276 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,328 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,368 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,420 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,472 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,526 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,577 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,629 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,682 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,734 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,779 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,832 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,867 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,918 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:54,971 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,027 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,079 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,121 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,173 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,220 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,273 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,316 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,369 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,396 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,448 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,501 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,549 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,601 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,655 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,707 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,760 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,812 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,868 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,920 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:55,972 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,025 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,076 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,127 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,161 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,214 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,256 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,308 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,360 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,412 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,465 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,518 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,570 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,625 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,677 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,732 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,783 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,835 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,887 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,938 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:56,990 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,042 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,079 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,134 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,186 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,238 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,290 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,342 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,395 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,448 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,500 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,553 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,608 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,663 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,716 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,767 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,819 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,871 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,923 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:57,976 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,027 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,079 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,132 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,184 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,237 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,290 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,342 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,379 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,430 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,485 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,539 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,604 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,648 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,702 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,756 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,810 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,846 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,901 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:58,954 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,006 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,061 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,115 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,169 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,233 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,289 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,344 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,399 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,454 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,476 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,533 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,586 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,637 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,691 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,720 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,743 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,781 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,837 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,891 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:14:59,950 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,005 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,058 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,112 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,166 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,222 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,278 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,332 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,389 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,444 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,500 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,556 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,612 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,669 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,724 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,777 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,834 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,888 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,943 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:00,996 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,050 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,105 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,129 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,155 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,187 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,211 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,235 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,286 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,323 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,360 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,396 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,431 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,469 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,498 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,527 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,579 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,632 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,685 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,749 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,803 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,842 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,896 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,938 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:01,992 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,047 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,089 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,144 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,198 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,264 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,315 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,370 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,418 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,472 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,528 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,583 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,638 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:02,683 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ee98f",
   "metadata": {},
   "source": [
    "## The RAG query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0298bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    \"Can you tell me how Alita and MCP Zero can interplay with each other? \"\n",
    "    \"Also, how can GEPA perform better than GRPO even though it's a prompt engineering \"\n",
    "    \"technique that does not rewrite the weights of the LLM?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ece21d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:15:09,831 - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "base_query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca1d0607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:15:10,694 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:14,821 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "response = await base_query_engine.aquery(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f6bcb17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Alita generates MCPs which can be utilized by other agents, enhancing their capabilities and problem-solving abilities. These MCPs, distilled from powerful models like Claude-3.7-Sonnet, bridge the gap in task-processing capabilities between agents utilizing smaller LLMs and those leveraging larger models."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d7b3989",
   "metadata": {},
   "outputs": [],
   "source": [
    "higher_k_query_engine = index.as_query_engine(similarity_top_k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5acce928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:15:19,922 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:26,321 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Alita‚Äôs design centers around leveraging the increasing coding and reasoning capabilities of LLMs. When running Alita on GAIA using GPT-4o-mini, it generates its own MCPs ‚Äì meaning it doesn‚Äôt rely on distilled MCPs from more powerful models like Claude-3.7-Sonnet. The experiment shows that Alita performs significantly worse on GAIA compared to when using GPT-4o-mini. This highlights the critical role of the underlying models‚Äô coding capabilities. \n",
       "\n",
       "GEPA can outperform GRPO, even though it‚Äôs a prompt engineering technique, because it uses a Pareto-based sampling strategy to generate prompts. This approach allows GEPA to explore a broader range of potential solutions and identify the most effective prompts, leading to improved performance. Furthermore, GEPA‚Äôs system-aware crossover strategies can provide large gains, but the optimal budget allocation between mutation and crossover, as well as when to invoke merge needs further study."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = await higher_k_query_engine.aquery(query)\n",
    "display(Markdown(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e8e8735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor.llm_rerank import LLMRerank\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "reranker = LLMRerank(top_n=4)\n",
    "reranker_query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever = index.as_retriever(similarity_top_k=10),\n",
    "    llm = Settings.llm,\n",
    "    node_postprocessors=[reranker]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0d6cf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:15:30,094 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:35,911 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:39,487 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Alita and MCP-Zero address complementary halves of the same problem: MCP-Zero efficiently finds and invokes existing tools, while Alita automatically builds missing tools on-the-fly. When combined, they form a virtuous loop where an agent first actively discovers tools, and if none fit, Alita synthesizes a new one. GEPA achieves optimal test set performance by rapidly adapting and generalizing in compound AI systems, outperforming GRPO by up to 19% while using up to 35x fewer rollouts."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = await reranker_query_engine.aquery(query)\n",
    "display(Markdown(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cde5827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "\n",
    "hyde = HyDEQueryTransform(include_original=True)\n",
    "hyde_query_engine = TransformQueryEngine(reranker_query_engine, query_transform=hyde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80ca573d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:15:54,153 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:54,238 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:54,266 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:15:59,439 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:16:02,741 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "According to the provided documents, Alita generates MCPs (Mission Control Protocols) which are then reused by other agents, like ODR-smolagents. Specifically, Alita, when using GPT-4o-mini, generates its own MCPs, unlike the experiment in Section 5.1.3 where the agent utilized MCPs distilled from Claude-3.7-Sonnet. GEPA, a prompt optimizer, can outperform GRPO, a reinforcement learning algorithm, because it incorporates natural language reflection to diagnose problems and propose prompt updates, leading to a significant quality gain with fewer rollouts."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = await hyde_query_engine.aquery(query)\n",
    "display(Markdown(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77dcbbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:16:06,838 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[alita-gepa-mcpZero] Q: What is the relationship between Alita and MCP Zero?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:16:07,729 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:16:07,781 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:16:07,806 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:16:15,592 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:16:18,973 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;237;90;200m[alita-gepa-mcpZero] A: Alita autonomously expands its capabilities through continuous MCP integration. It generates MCPs, which are then encapsulated and stored in the MCP Box for future reuse. These MCPs are created through a self-reinforcing cycle where Alita continuously integrates new MCPs, enhancing its overall capabilities.\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[alita-gepa-mcpZero] Q: How do Alita and MCP Zero interact?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:16:19,885 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:16:19,935 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:16:19,961 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:01,261 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;90;149;237m[alita-gepa-mcpZero] A: Empty Response\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[alita-gepa-mcpZero] Q: How does GEPA improve GRPO performance?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:17:02,453 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:02,505 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:02,530 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:08,339 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:11,661 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;11;159;203m[alita-gepa-mcpZero] A: GEPA achieves superior test set performance compared to GRPO on tasks like HotpotQA, IFBench, HoVer, and PUPA by requiring significantly fewer rollouts. Specifically, it matches GRPO‚Äôs best validation scores with 402, 330, 1179, and 306 rollouts, respectively, while achieving up to 78 times greater sample efficiency. Furthermore, the combined GEPA+Merge approach out-performs GRPO by an even wider margin of 21% at a comparable rollout budget.\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:17:12,754 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Alita autonomously expands its capabilities through continuous integration of MCPs. It generates these MCPs and stores them for later use. This creates a cycle of enhancement. GEPA achieves better results than GRPO by using far fewer rollouts to reach similar validation scores, demonstrating greater sample efficiency."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.question_gen import LLMQuestionGenerator\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=hyde_query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"alita-gepa-mcpZero\",\n",
    "            description=\"Use this for specific questions relating to alita, gepa and/or mcp zero\",\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "generator = LLMQuestionGenerator.from_defaults()\n",
    "sub_question_query_engine = SubQuestionQueryEngine(\n",
    "    question_gen=generator,\n",
    "    response_synthesizer=get_response_synthesizer(),\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    use_async=False\n",
    ")\n",
    "\n",
    "response = sub_question_query_engine.query(query)\n",
    "display(Markdown(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15b7b115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.query.query_transform import (\n",
    "    StepDecomposeQueryTransform\n",
    ")\n",
    "from llama_index.core.query_engine import MultiStepQueryEngine\n",
    "\n",
    "transform = StepDecomposeQueryTransform(verbose=True)\n",
    "multi_step_query_engine = MultiStepQueryEngine(\n",
    "    query_engine = sub_question_query_engine,\n",
    "    query_transform = transform,\n",
    "    index_summary = \"Answers questions relating to alita, gepa, and/or mcp zero.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3a848e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:17:21,853 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;33m> Current query: Can you tell me how Alita and MCP Zero can interplay with each other? Also, how can GEPA perform better than GRPO even though it's a prompt engineering technique that does not rewrite the weights of the LLM?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: How does MCP Zero interact with Alita?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:17:22,928 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[alita-gepa-mcpZero] Q: What is the interaction between Alita and MCP Zero?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:17:24,004 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:24,058 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:24,085 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:30,982 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;237;90;200m[alita-gepa-mcpZero] A: Empty Response\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:17:31,315 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:32,016 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;33m> Current query: Can you tell me how Alita and MCP Zero can interplay with each other? Also, how can GEPA perform better than GRPO even though it's a prompt engineering technique that does not rewrite the weights of the LLM?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: How does MCP Zero interact with Alita?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:17:33,121 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[alita-gepa-mcpZero] Q: What is the interaction between Alita and MCP Zero?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:17:34,331 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:34,387 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:34,414 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:41,863 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;237;90;200m[alita-gepa-mcpZero] A: Empty Response\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:17:42,187 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:42,961 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;33m> Current query: Can you tell me how Alita and MCP Zero can interplay with each other? Also, how can GEPA perform better than GRPO even though it's a prompt engineering technique that does not rewrite the weights of the LLM?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: How does MCP Zero interact with Alita?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:17:44,183 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[alita-gepa-mcpZero] Q: What is the interaction between Alita and MCP Zero?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:17:45,468 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:45,523 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:45,552 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:53,328 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;237;90;200m[alita-gepa-mcpZero] A: Empty Response\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 20:17:53,727 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-05 20:17:54,016 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Empty Response\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = multi_step_query_engine.query(query)\n",
    "display(Markdown(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b5db536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import RichPromptTemplate\n",
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    step,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Event,\n",
    "    Workflow\n",
    ")\n",
    "from pydantic import BaseModel\n",
    "from typing import Annotated, Optional\n",
    "\n",
    "check_answer_template = RichPromptTemplate(\"\"\"\n",
    "{% chat role = \"user\" %}\n",
    "This is the original question: \n",
    "<question>\n",
    "    {{ question }}\n",
    "</question>\n",
    "\n",
    "Here are the following questions we have asked:\n",
    "<follow_up_questions>\n",
    "    {{ follow_up_questions }}\n",
    "</follow_up_questions>\n",
    "\n",
    "Here is our current answer:\n",
    "<answer>\n",
    "    {{ answer }}\n",
    "</answer>\n",
    "\n",
    "Does the current answer address the original question? If not, generate\n",
    "a follow-up question such that including the answer to this follow-up question\n",
    "to the current answer we have so far answers the user's original question.\n",
    "{% endchat %}\n",
    "\"\"\")\n",
    "\n",
    "class ShouldContinue(BaseModel):\n",
    "    should_continue: bool\n",
    "    reasoning: Annotated[str, \"Whether the current answer answers the question\"]\n",
    "\n",
    "class ConsolidateEvent(Event):\n",
    "    original_question: str\n",
    "    current_response: str\n",
    "    new_response: str\n",
    "    follow_up_questions: list[Optional[str]]\n",
    "\n",
    "class CheckAnswerEvent(Event):\n",
    "    original_question: str\n",
    "    follow_up_questions: list[Optional[str]]\n",
    "    response: str\n",
    "\n",
    "class ContinueEvent(Event):\n",
    "    original_question: str\n",
    "    current_answer: str\n",
    "    follow_up_questions: list[Optional[str]]\n",
    "    reason_to_continue: str\n",
    "\n",
    "class AskQueryEvent(Event):\n",
    "    query: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d86db60",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Settings.llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb482561",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStepRAG(Workflow):    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.llm = Settings.llm        \n",
    "    \n",
    "    @step\n",
    "    async def query_step(\n",
    "        self, ctx: Context, ev: StartEvent | AskQueryEvent\n",
    "    ) -> CheckAnswerEvent | ConsolidateEvent:\n",
    "        \n",
    "        query = ev.get('query')\n",
    "        follow_up_questions = ev.get('follow_up_questions', [])\n",
    "        response = sub_question_query_engine.query(query)\n",
    "        current_response = await ctx.store.get(\"current_answer\", None)\n",
    "        \n",
    "        if current_response:\n",
    "            return ConsolidateEvent(\n",
    "                original_question=query,\n",
    "                current_response=current_response,\n",
    "                new_response=response.response,\n",
    "                follow_up_questions=follow_up_questions\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            await ctx.store.set(\"current_answer\", current_response)\n",
    "            return CheckAnswerEvent(\n",
    "                original_question = query,\n",
    "                follow_up_questions=follow_up_questions,\n",
    "                response = response.response\n",
    "            )\n",
    "    \n",
    "    @step\n",
    "    async def consolidate_response(\n",
    "        self, ctx: Context, ev: ConsolidateEvent\n",
    "    ) -> CheckAnswerEvent:\n",
    "        \n",
    "        follow_up_questions = ev.get('follow_up_questions')\n",
    "        original_question = ev.get('original_question')\n",
    "        current_response = ev.get('current_response')\n",
    "        new_response = ev.get('new_response')\n",
    "        response = await self.llm.acomplete(\n",
    "            f\"\"\"\n",
    "            This is the question we're trying to answer: {original_question}\n",
    "            \n",
    "            Here is the answer we have so far: {current_response}\n",
    "            \n",
    "            This is an additional component to make our current answer more complete: {new_response}\n",
    "            \n",
    "            Generate a coherent answer based on our current answer and the additional component.\n",
    "            \"\"\"\n",
    "        )\n",
    "        await ctx.store.set(\"current_answer\", response.text)\n",
    "        print(current_response)\n",
    "        print(original_question)\n",
    "        print(follow_up_questions)\n",
    "        \n",
    "        return CheckAnswerEvent(\n",
    "            original_question = query,\n",
    "            follow_up_questions=follow_up_questions,\n",
    "            response = response.text\n",
    "        )\n",
    "    \n",
    "    @step\n",
    "    async def check_answer_step(\n",
    "        self, ctx: Context, ev: CheckAnswerEvent\n",
    "    ) -> ContinueEvent | StopEvent:\n",
    "        original_question = ev.get('original_question')\n",
    "        follow_up_questions = ev.get('follow_up_questions')\n",
    "        current_answer = ev.get(\"response\")\n",
    "        check_answer_template.format(\n",
    "            question=original_question, \n",
    "            follow_up_questions = follow_up_questions,\n",
    "            answer = current_answer\n",
    "        )\n",
    "        result = self.llm.structured_predict(ShouldContinue, check_answer_template)\n",
    "        print(result)\n",
    "        if result.should_continue:\n",
    "            return ContinueEvent(\n",
    "                current_answer = current_answer,\n",
    "                original_question = original_question,\n",
    "                follow_up_questions = follow_up_questions,\n",
    "                reason_to_continue = result.reasoning\n",
    "            )\n",
    "        return StopEvent(result = current_answer)\n",
    "    \n",
    "    @step \n",
    "    async def generate_follow_up_question(\n",
    "        self, ctx: Context, ev: ContinueEvent\n",
    "    ) -> AskQueryEvent:\n",
    "        original_question = ev.get(\"original_question\")\n",
    "        current_response = ev.get(\"current_answer\")\n",
    "        \n",
    "        result = await llm.acomplete(\n",
    "             f\"\"\"\n",
    "            This is the question we're trying to answer: {original_question}\n",
    "            \n",
    "            Here is the answer we have so far: {current_response}\n",
    "            \n",
    "            We've not fully addressed the question yet. Generate a follow-up question\n",
    "            so that the answer to this question will address the original question once\n",
    "            combined with our current response.\n",
    "            \"\"\"\n",
    "        )\n",
    "        return AskQueryEvent(query = result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a002ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_step_query_engine.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(\n",
    "    MultiStepRAG(),\n",
    "    filename=\"multi_step_query_engine.html\",\n",
    "    # Optional, can limit long event names in your workflow\n",
    "    # Can help with readability\n",
    "    # max_label_length=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "de880364",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_step_query_engine = MultiStepRAG(timeout=6000)\n",
    "ctx = Context(multi_step_query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "106bd409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 07:17:06,009 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[alita-gepa-mcpZero] Q: What is the relationship between Alita and MCP Zero?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 07:17:06,895 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-06 07:17:06,941 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-06 07:17:06,971 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-06 07:17:14,318 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-06 07:17:17,370 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;237;90;200m[alita-gepa-mcpZero] A: Alita autonomously expands its capabilities through continuous MCP integration. It generates MCPs, which are then encapsulated and stored in the MCP Box for future reuse. These MCPs are created through a self-reinforcing cycle where Alita continuously integrates new MCPs, enhancing its overall capabilities.\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[alita-gepa-mcpZero] Q: How do Alita and MCP Zero interact?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 07:17:18,273 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-06 07:17:18,327 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-06 07:17:18,356 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-06 07:17:57,887 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;90;149;237m[alita-gepa-mcpZero] A: Empty Response\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[alita-gepa-mcpZero] Q: How does GEPA improve GRPO performance?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 07:17:59,058 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-06 07:17:59,110 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-06 07:17:59,140 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-06 07:18:05,137 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-06 07:18:08,506 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;11;159;203m[alita-gepa-mcpZero] A: GEPA achieves superior test set performance compared to GRPO on tasks like HotpotQA, IFBench, HoVer, and PUPA by requiring significantly fewer rollouts. Specifically, it matches GRPO‚Äôs best validation scores with 402, 330, 1179, and 306 rollouts, respectively, while achieving up to 78 times greater sample efficiency. Furthermore, the combined GEPA+Merge approach out-performs GRPO by an even wider margin of 21% at a comparable rollout budget.\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 07:18:09,619 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-11-06 07:18:11,083 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "The current answer is empty. Therefore, it does not address the original question, which is missing. To address the original question, we need a follow-up question that prompts the user to provide an answer. A suitable follow-up question is: 'Please provide your answer to the original question.' \n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "3 validation errors for ContinueEvent\noriginal_question\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncurrent_answer\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\nfollow_up_questions\n  Input should be a valid list [type=list_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.12/v/list_type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m multi_step_query_engine.run(query=query)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.4/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/futures.py:286\u001b[39m, in \u001b[36mFuture.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    285\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncio_future_blocking = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mawait wasn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt used with future\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.4/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py:375\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    377\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    378\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.4/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/futures.py:199\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/Observe/.venv/lib/python3.13/site-packages/workflows/runtime/broker.py:156\u001b[39m, in \u001b[36mWorkflowBroker.start.<locals>._run_workflow\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    147\u001b[39m workflow_registry.register_run(\n\u001b[32m    148\u001b[39m     run_id=run_id,\n\u001b[32m    149\u001b[39m     workflow=workflow,\n\u001b[32m   (...)\u001b[39m\u001b[32m    152\u001b[39m     steps=registered.steps,\n\u001b[32m    153\u001b[39m )\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     workflow_result = \u001b[38;5;28;01mawait\u001b[39;00m registered.workflow_function(\n\u001b[32m    157\u001b[39m         start_event,\n\u001b[32m    158\u001b[39m         init_state,\n\u001b[32m    159\u001b[39m         run_id,\n\u001b[32m    160\u001b[39m     )\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# ensure run context is cleaned up even on failure\u001b[39;00m\n\u001b[32m    163\u001b[39m     workflow_registry.delete_run(run_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/Observe/.venv/lib/python3.13/site-packages/workflows/runtime/control_loop.py:302\u001b[39m, in \u001b[36mcontrol_loop\u001b[39m\u001b[34m(start_event, init_state, run_id)\u001b[39m\n\u001b[32m    298\u001b[39m state = init_state \u001b[38;5;129;01mor\u001b[39;00m BrokerState.from_workflow(current.workflow)\n\u001b[32m    299\u001b[39m runner = _ControlLoopRunner(\n\u001b[32m    300\u001b[39m     current.workflow, current.plugin, current.context, current.steps, state\n\u001b[32m    301\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m runner.run(start_event=start_event)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/Observe/.venv/lib/python3.13/site-packages/workflows/runtime/control_loop.py:275\u001b[39m, in \u001b[36m_ControlLoopRunner.run\u001b[39m\u001b[34m(self, start_event, start_with_timeout)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m command \u001b[38;5;129;01min\u001b[39;00m commands:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_command(command)\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    277\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cleanup_tasks()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/Observe/.venv/lib/python3.13/site-packages/workflows/runtime/control_loop.py:202\u001b[39m, in \u001b[36m_ControlLoopRunner.process_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(command, CommandFailWorkflow):\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cleanup_tasks()\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m command.exception\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown command type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(command)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/Observe/.venv/lib/python3.13/site-packages/workflows/runtime/types/step_function.py:120\u001b[39m, in \u001b[36mas_step_worker_function.<locals>.wrapper\u001b[39m\u001b[34m(state, step_name, event, context, workflow)\u001b[39m\n\u001b[32m    115\u001b[39m     result: R = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.get_event_loop().run_in_executor(\n\u001b[32m    116\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    117\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: copy.run(partial_func),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    118\u001b[39m     )\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m partial_func()\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Event):\n\u001b[32m    122\u001b[39m         msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStep function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m instead of an Event instance.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/Observe/.venv/lib/python3.13/site-packages/llama_index_instrumentation/dispatcher.py:386\u001b[39m, in \u001b[36mDispatcher.span.<locals>.async_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28mself\u001b[39m.span_enter(\n\u001b[32m    379\u001b[39m     id_=id_,\n\u001b[32m    380\u001b[39m     bound_args=bound_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m     tags=tags,\n\u001b[32m    384\u001b[39m )\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m.event(SpanDropEvent(span_id=id_, err_str=\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36mMultiStepRAG.check_answer_step\u001b[39m\u001b[34m(self, ctx, ev)\u001b[39m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28mprint\u001b[39m(follow_up_questions)\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mprint\u001b[39m(result.reasoning)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mContinueEvent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcurrent_answer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_answer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_question\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_question\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_up_questions\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_up_questions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreason_to_continue\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreasoning\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m StopEvent(result = current_answer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/Observe/.venv/lib/python3.13/site-packages/workflows/events.py:148\u001b[39m, in \u001b[36mEvent.__init__\u001b[39m\u001b[34m(self, **params)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, **params: Any):\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/Observe/.venv/lib/python3.13/site-packages/workflows/events.py:52\u001b[39m, in \u001b[36mDictLikeModel.__init__\u001b[39m\u001b[34m(self, **params)\u001b[39m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     51\u001b[39m         data[k] = v\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m private_attr, value \u001b[38;5;129;01min\u001b[39;00m private_attrs.items():\n\u001b[32m     54\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__setattr__\u001b[39m(private_attr, value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/Observe/.venv/lib/python3.13/site-packages/pydantic/main.py:250\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    249\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    252\u001b[39m     warnings.warn(\n\u001b[32m    253\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    254\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    256\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    257\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 3 validation errors for ContinueEvent\noriginal_question\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\ncurrent_answer\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\nfollow_up_questions\n  Input should be a valid list [type=list_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.12/v/list_type"
     ]
    }
   ],
   "source": [
    "result = await multi_step_query_engine.run(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b9d844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e39e8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here‚Äôs how the two topics fit together, based on the provided material:\n",
       "\n",
       "- Interplay between Alita and MCP-Zero\n",
       "  - What each does\n",
       "    - MCP-Zero is a tool-discovery engine: it actively searches for existing tools and capabilities across resources, and invokes them when suitable. It focuses on maximizing tool discovery and reuse.\n",
       "    - Alita is a generalist agent framework that evolves capabilities by generating and refining task-related model context protocols (MCPs) from open-source material. It aims to synthesize and reuse external capabilities with minimal upfront handcrafting.\n",
       "  - How they work together\n",
       "    - They form a complementary loop: first, MCP-Zero tries to find and invoke existing tools to tackle the agent‚Äôs tasks.\n",
       "    - If no suitable tool is found, Alita‚Äôs workflow can be engaged to synthesize a new tool by generating a new MCP tailored to the task, effectively creating new capabilities.\n",
       "    - The newly created tool (and its MCP) can then be registered and made available to the community, enriching the tool ecosystem for future tasks.\n",
       "  - Why this is powerful\n",
       "    - This pairing balances discovery and creation: MCP-Zero maximizes what already exists, while Alita drives scalable self-evolution by producing and integrating new tools via MCPs.\n",
       "    - The combination supports broader generalization across domains: semantic grounding via MCPs helps clarify tool semantics, enabling reliable tool use and faster adaptation to new tasks.\n",
       "\n",
       "- Why GEPA can beat GRPO without changing LLM weights\n",
       "  - Core idea\n",
       "    - GEPA is a reflective prompt evolution method that optimizes prompts (system-level instructions and tool-use guidance) rather than updating model weights. It leverages natural-language reflection to diagnose issues, propose prompt updates, and combine lessons from multiple attempts.\n",
       "  - Why it can outperform weight-space RL (GRPO)\n",
       "    - High sample efficiency: GEPA can achieve large performance gains with far fewer rollouts (up to 35x fewer) by learning mainly from improved prompts and reflections rather than policy updates.\n",
       "    - Better use of feedback: GEPA uses a reflection-based process to generate high-quality, task-relevant learning signals from each rollout, guiding prompt evolution more effectively than scalar reward signals alone.\n",
       "    - Diverse, Pareto-guided exploration: GEPA uses Pareto-based candidate sampling to maintain diversity among evolving prompts, avoiding local optima that can trap strategies that always pick the current best candidate.\n",
       "    - Systematic prompt combination: The approach includes mutation and a system-aware merge step, which can combine complementary prompt strategies from different evolutionary lineages to produce stronger prompts.\n",
       "    - Evidence across tasks/models: In experiments, GEPA and its variant GEPA+Merge outperformed GRPO by up to about 19% on some tasks, with substantial reductions in rollouts required, and often matched or exceeded GRPO‚Äôs best validation scores with far fewer learning signals.\n",
       "  - Practical takeaway\n",
       "    - The gains come from optimizing the prompts and the learning dynamics (how prompts are mutated, merged, and selected) rather than from changing LLM weights. This makes GEPA a highly sample-efficient way to improve downstream performance for complex, modular AI systems where prompts and system behavior are crucial.\n",
       "\n",
       "If you want, I can summarize how to architect a system that combines Alita with MCP-Zero in a concrete workflow, and separately outline a GEPA-inspired prompt-evolution protocol you could pilot for a given task."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "openai_query_engine = index.as_query_engine(similariry_top_k=20, llm=OpenAI(model=\"gpt-5-nano\", temperature=0))\n",
    "display(Markdown(response.response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Observe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
